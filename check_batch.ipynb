{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dist_path = '/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/ptbmodel/'\n",
    "latest_point_path = os.path.join(dist_path, 'results/test_min.pkl')\n",
    "train_bs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "12600\n",
      "Load epoch 5788 succeed！\n",
      "Load train loss succed！\n",
      "Load test loss succed！\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import utils\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from model import WHOLEMODEL\n",
    "from batch import GGCNNDATASET\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl import batch\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings('ignore')  # 忽略所有警告\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "seed = 1 # seed必须是int，可以自行设置\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) # 让显卡产生的随机数一致\n",
    "torch.cuda.manual_seed_all(seed) # 多卡模式下，让所有显卡生成的随机数一致？这个待验证\n",
    "np.random.seed(seed) # numpy产生的随机数一致\n",
    "random.seed(seed) # python产生的随机数一致\n",
    "\n",
    "# CUDA中的一些运算，如对sparse的CUDA张量与dense的CUDA张量调用torch.bmm()，它通常使用不确定性算法。\n",
    "# 为了避免这种情况，就要将这个flag设置为True，让它使用确定的实现。\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 设置这个flag可以让内置的cuDNN的auto-tuner自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。\n",
    "# 但是由于噪声和不同的硬件条件，即使是同一台机器，benchmark都可能会选择不同的算法。为了消除这个随机性，设置为 False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "train_data_path = os.path.join(dist_path,'datas/train_data')\n",
    "test_data_path = os.path.join(dist_path,'datas/test_data')\n",
    "config_json_file = os.path.join(dist_path, 'datas/config.json')\n",
    "if not os.path.exists(os.path.join(dist_path, 'results')):\n",
    "    os.makedirs(os.path.join(dist_path, 'results'), exist_ok=True)\n",
    "\n",
    "with open(config_json_file, 'r', encoding='utf-8') as f:\n",
    "    config_para = json.load(f)\n",
    "\n",
    "# configure hyper parameters\n",
    "train_num      = config_para['train_num']\n",
    "test_num       = config_para['test_num']\n",
    "train_reload   = config_para['train_reload']\n",
    "test_reload    = config_para['test_reload']\n",
    "batch_size     = config_para['batch_size']\n",
    "num_epoch      = config_para['num_epoch']\n",
    "lr_radio_init  = config_para['lr_radio_init']\n",
    "lr_factor      = config_para['lr_factor']\n",
    "lr_patience    = config_para['lr_patience']\n",
    "lr_verbose     = config_para['lr_verbose']\n",
    "lr_threshold   = config_para['lr_threshold']\n",
    "lr_eps         = config_para['lr_eps']\n",
    "min_lr         = config_para['min_lr']\n",
    "cooldown       = config_para['cooldown']\n",
    "is_sch         = config_para['is_sch']\n",
    "is_save        = config_para['is_save']\n",
    "save_frequncy  = config_para['save_frequncy']\n",
    "\n",
    "is_L1          = config_para['is_L1']\n",
    "is_L2          = config_para['is_L2']\n",
    "L1_radio       = config_para['L1_radio']\n",
    "L2_radio       = config_para['L2_radio']\n",
    "\n",
    "reset_all        = config_para['reset_all']\n",
    "reset_model      = config_para['reset_model']\n",
    "reset_model_path = config_para['model_path']\n",
    "reset_opt        = config_para['reset_opt']\n",
    "reset_sch        = config_para['reset_sch']\n",
    "\n",
    "# configure trainingset path\n",
    "trainset_rawdata_path = os.path.join(train_data_path, 'raw')\n",
    "trainset_dgldata_path = os.path.join(train_data_path, 'dgl')\n",
    "\n",
    "# configure trainingset path\n",
    "testset_rawdata_path = os.path.join(test_data_path, 'raw')\n",
    "testset_dgldata_path = os.path.join(test_data_path, 'dgl')\n",
    "\n",
    "# configure network structure\n",
    "embedding_dim          = config_para['embedding_dim']\n",
    "index_dim              = config_para['index_dim']\n",
    "graph_dim              = config_para['graph_dim']\n",
    "gnn_dim_list           = config_para['gnn_dim_list']\n",
    "gnn_head_list          = config_para['gnn_head_list']\n",
    "onsite_dim_list1       = config_para['onsite_dim_list1']\n",
    "onsite_dim_list2       = config_para['onsite_dim_list2']\n",
    "orb_dim_list           = config_para['orb_dim_list']\n",
    "hopping_dim_list1      = config_para['hopping_dim_list1']\n",
    "hopping_dim_list2      = config_para['hopping_dim_list2']\n",
    "expander_bessel_dim    = config_para['expander_bessel_dim']\n",
    "expander_bessel_cutoff = config_para['expander_bessel_cutoff']\n",
    "atom_num               = config_para['atom_num']\n",
    "is_orb                 = config_para['is_orb']\n",
    "\n",
    "utils.seed_torch(seed = 24)\n",
    "\n",
    "trainset, traininfos = utils.get_data(\n",
    "                                        raw_dir = trainset_rawdata_path, \n",
    "                                        save_dir = trainset_dgldata_path,\n",
    "                                        data_num = train_num, \n",
    "                                        force_reload = train_reload,\n",
    "                                        )\n",
    "\n",
    "traingraphs, trainlabels, init_dim = trainset.get_all()\n",
    "# traingraphs = batch(traingraphs)\n",
    "# traingraphs = traingraphs.to(device)\n",
    "train_dataloader = GraphDataLoader(trainset, batch_size = train_bs, drop_last = False, shuffle = False)\n",
    "\n",
    "with open(os.path.join(dist_path, 'train_infos.txt'), 'w+') as file:\n",
    "    for i in traininfos.values():\n",
    "        file.write(i['filename'] + '\\n')\n",
    "\n",
    "testset, testinfos = utils.get_data(\n",
    "                                    raw_dir = testset_rawdata_path, \n",
    "                                    save_dir = testset_dgldata_path, \n",
    "                                    data_num = test_num,\n",
    "                                    force_reload = test_reload,\n",
    "                                    )\n",
    "\n",
    "testgraphs, testlabels, init_dim = testset.get_all()\n",
    "# testgraphs = batch(testgraphs)\n",
    "# testgraphs = testgraphs.to(device)\n",
    "test_dataloader = GraphDataLoader(testset, batch_size = 1, drop_last = False, shuffle = False)\n",
    "\n",
    "with open(os.path.join(dist_path, 'test_infos.txt'), 'w+') as file:\n",
    "    for i in testinfos.values():\n",
    "        file.write(i['filename'] + '\\n')\n",
    "\n",
    "model = WHOLEMODEL(\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    index_dim = index_dim,\n",
    "                    graph_dim = graph_dim,\n",
    "                    gnn_dim_list = gnn_dim_list,\n",
    "                    gnn_head_list = gnn_head_list,\n",
    "                    orb_dim_list = orb_dim_list,\n",
    "                    onsite_dim_list1 = onsite_dim_list1,\n",
    "                    onsite_dim_list2 = onsite_dim_list2,\n",
    "                    hopping_dim_list1 = hopping_dim_list1,\n",
    "                    hopping_dim_list2 = hopping_dim_list2,\n",
    "                    expander_bessel_dim = expander_bessel_dim,\n",
    "                    expander_bessel_cutoff = expander_bessel_cutoff,\n",
    "                    atom_num=atom_num*batch_size,\n",
    "                    is_orb = is_orb\n",
    "                    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr_radio_init, eps=lr_eps)\n",
    "sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=lr_factor, patience=lr_patience*int(train_num / batch_size), verbose=lr_verbose, threshold=lr_threshold, threshold_mode='rel', cooldown=cooldown*int(train_num / batch_size), min_lr=min_lr, eps=lr_eps)\n",
    "\n",
    "print(lr_patience*int(train_num / batch_size))\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "loss_per_epoch = np.zeros(int(train_num / train_bs))  \n",
    "losses = np.zeros(num_epoch)\n",
    "test_losses = np.zeros(num_epoch)\n",
    "\n",
    "if os.path.exists(latest_point_path) and not reset_all:\n",
    "    checkpoint = torch.load(latest_point_path)\n",
    "    if not reset_model:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        checkpoint = torch.load(reset_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if not reset_opt:\n",
    "        opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if not reset_sch:\n",
    "        sch.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    loss = checkpoint['loss']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print('Load epoch {} succeed！'.format(start_epoch))\n",
    "    if os.path.exists(os.path.join(dist_path, 'results/losses.npy')):\n",
    "        losses = np.load(os.path.join(dist_path, 'results/losses.npy'))\n",
    "        if num_epoch > losses.size:\n",
    "            losses = np.concatenate((losses, np.zeros(num_epoch - losses.size)))\n",
    "        print('Load train loss succed！')\n",
    "    else:\n",
    "        losses = np.zeros(num_epoch)\n",
    "    if os.path.exists(os.path.join(dist_path, 'results/test_losses.npy')):\n",
    "        test_losses = np.load(os.path.join(dist_path, 'results/test_losses.npy'))\n",
    "        if num_epoch > test_losses.size:\n",
    "            test_losses = np.concatenate((test_losses, np.zeros(num_epoch - test_losses.size)))\n",
    "        print('Load test loss succed！')\n",
    "    else:\n",
    "        test_losses = np.zeros(num_epoch)\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    losses = np.zeros(num_epoch)\n",
    "    test_losses = np.zeros(num_epoch)\n",
    "    print('Can not load saved model!Training from beginning!')\n",
    "\n",
    "para_sk, hopping_index, hopping_info, d, is_hopping, onsite_key, cell_atom_num, onsite_num, orb1_index, orb2_index, orb_num, rvectors, rvectors_all, tensor_E, tensor_eikr, orb_key, filename = utils.batch_index(train_dataloader, traininfos, train_bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for dimension 0 with size 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m      9\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(labels[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m train_bs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# kkk.append(graphs.edata['distance'].cpu())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# def draw_dgl_graph(graph):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(graphs)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m hsk, feat, feato, featall, o, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpara_sk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_hopping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhopping_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morb_key\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monsite_key\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_atom_num\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monsite_num\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morb1_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morb2_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(d[i].flatten())\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# pltd.append(d[i].flatten().cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if (i+1)%4 == 0:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     plt.plot(np.concatenate(pltd))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     plt.show()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m b1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(hsk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels))\n",
      "File \u001b[0;32m~/Tools/anaconda3/envs/matformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/ptbmodel/model.py:508\u001b[0m, in \u001b[0;36mWHOLEMODEL.forward\u001b[0;34m(self, bg, para_sk, is_hopping, hopping_index, orb_key, d, onsite_key, cell_atom_num, onsite_num, orb1_index, orb2_index)\u001b[0m\n\u001b[1;32m    504\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn(bg, feato)\n\u001b[1;32m    506\u001b[0m spdffeats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspdfnn(feat)\n\u001b[0;32m--> 508\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monsite_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monsite_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# h = self.hnn(feat, hopping_index, d, self.expander(d), orb_key)\u001b[39;00m\n\u001b[1;32m    510\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhnn(spdffeats, hopping_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matom_num, orb_key, d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpander(d), orb1_index, orb2_index)\n",
      "File \u001b[0;32m~/Tools/anaconda3/envs/matformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/ptbmodel/model.py:276\u001b[0m, in \u001b[0;36mOnsiteNN.forward\u001b[0;34m(self, ofeat, onsite_key, onsite_num)\u001b[0m\n\u001b[1;32m    273\u001b[0m feat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(onsite_key[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize):\n\u001b[0;32m--> 276\u001b[0m     feat\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monsite_mlp1[onsite_key[\u001b[38;5;241m2\u001b[39m][i]](\u001b[43mofeat\u001b[49m\u001b[43m[\u001b[49m\u001b[43monsite_key\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m    278\u001b[0m feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(feat)\n\u001b[1;32m    280\u001b[0m onsite[onsite_key[\u001b[38;5;241m3\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monsite_mlp2(torch\u001b[38;5;241m.\u001b[39mcat((feat[onsite_key[\u001b[38;5;241m1\u001b[39m][:,\u001b[38;5;241m0\u001b[39m]], feat[onsite_key[\u001b[38;5;241m1\u001b[39m][:,\u001b[38;5;241m1\u001b[39m]]),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for dimension 0 with size 15"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "sss = []\n",
    "# kkk = []\n",
    "for epoch in range(start_epoch + 1, start_epoch + 2):\n",
    "    for graphs, labels in test_dataloader:\n",
    "        loss = 0\n",
    "        i = int(labels[0] / train_bs)\n",
    "\n",
    "        # kkk.append(graphs.edata['distance'].cpu())\n",
    "\n",
    "        # def draw_dgl_graph(graph):\n",
    "        #     # 将DGL图转换为networkx图\n",
    "        #     nx_graph = graph.to_networkx()\n",
    "\n",
    "        #     # 绘制networkx图\n",
    "        #     nx.draw(nx_graph, with_labels=True)\n",
    "\n",
    "        #     # 显示图形\n",
    "        #     plt.show()\n",
    "            \n",
    "        # draw_dgl_graph(graphs.cpu())\n",
    "\n",
    "        # print(graphs)\n",
    "        hsk, feat, feato, featall, o, h = model(graphs, para_sk[i], is_hopping[i], hopping_index[i], orb_key[i], d[i], onsite_key[i], cell_atom_num[i], onsite_num[i].sum(), orb1_index[i], orb2_index[i])\n",
    "        \n",
    "        # print(d[i].flatten())\n",
    "        # pltd.append(d[i].flatten().cpu().detach().numpy())\n",
    "        # if (i+1)%4 == 0:\n",
    "        #     plt.plot(np.concatenate(pltd))\n",
    "        #     plt.show()\n",
    "\n",
    "        b1 = int(hsk.shape[0] / len(labels))\n",
    "        b2 = int(hopping_info[i].shape[0] / len(labels))\n",
    "        b3 = int(orb_num[i].shape[0] / len(labels))\n",
    "        b4 = int(cell_atom_num[i] / len(labels))\n",
    "\n",
    "        # kkk.append(d[i])\n",
    "        for j in range(len(labels)):\n",
    "            sss.append(d[i][j * 980:(j + 1)*980])\n",
    "            # print(featall[j * 15:(j + 1 )*15])\n",
    "            HR = utils.construct_hr(hsk[j * b1:(j + 1) * b1], hopping_info[i][j * b2:(j + 1) * b2], orb_num[i][j * b3:(j + 1) * b3], b4, rvectors[i][j])\n",
    "            reproduced_bands = utils.compute_bands(HR, tensor_eikr[i][j])\n",
    "            loss += criterion(reproduced_bands[:, 4:12], tensor_E[i][j][:, 4:12])\n",
    "            reproduced_band = reproduced_bands.cpu().detach().numpy()[:, 4:12]\n",
    "\n",
    "            reference_bands = tensor_E[i][j].cpu().detach().numpy()[:, 4:12]\n",
    "\n",
    "            # plt.imshow(feato[j * 15:(j + 1 )*15].cpu().detach().numpy(), aspect='auto')\n",
    "            # plt.colorbar()\n",
    "\n",
    "            # f = plt.gcf()  #获取当前图像\n",
    "            # f.savefig('/media/pigz2538/E2BCC52DBCC4FCD3/Users/z2538/Desktop/111/MoS2-test8/feats.png')\n",
    "            # plt.show()\n",
    "            # plt.imshow(HR[0].cpu().detach().numpy())\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "\n",
    "            for k in range(reproduced_band.shape[1]):\n",
    "                plt.plot(reference_bands[:,k],color = 'r')\n",
    "                plt.plot(reproduced_band[:,k],color = 'b', ls=':')\n",
    "\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('eV')\n",
    "            plt.legend(['DFT', 'TBGAT'])\n",
    "            # plt.savefig('/home/pigz2538/桌面/20240330/' + predict_name[:-5] + '-4090.png')\n",
    "            plt.show()\n",
    "\n",
    "        if is_L1:\n",
    "            L1 = 0\n",
    "            for name,param in model.named_parameters():\n",
    "                if 'bias' not in name:\n",
    "                    L1 += torch.norm(param, p=1) * L1_radio\n",
    "            loss += L1\n",
    "\n",
    "        if is_L2:\n",
    "            L2 = 0\n",
    "            for name,param in model.named_parameters():\n",
    "                if 'bias' not in name:\n",
    "                    L2 += torch.norm(param, p=2) * L2_radio\n",
    "            loss += L2\n",
    "        \n",
    "        if is_sch:\n",
    "            sch.step(loss)\n",
    "\n",
    "        loss_per_epoch[i] = loss.item()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    # print(loss_per_epoch)\n",
    "    # print(test_loss)\n",
    "    losses[epoch - 1] = loss_per_epoch.sum() / train_num\n",
    "    current_lr = opt.param_groups[0]['lr']\n",
    "\n",
    "    print(\"Epoch {:05d} | Train_Loss {:.6f} | Learning_rate {:.6f}\" . format(epoch, losses[epoch - 1], current_lr))\n",
    "\n",
    "print('trainging OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,1) (980,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m batch4in\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# batch1out.shape\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# batch4out[2940:]-batch1out\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# orb2_indexs[0][1]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# orb2_indexk[0][1]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mbatch4in\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2940\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbatch1in\u001b[49m\n\u001b[1;32m     12\u001b[0m s\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,1) (980,1) "
     ]
    }
   ],
   "source": [
    "batch1in = np.load('/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/tbmodel/batch1in.npy')\n",
    "batch1out = np.load('/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/tbmodel/batch1out.npy')\n",
    "batch4in = np.load('/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/tbmodel/batch4in.npy')\n",
    "batch4out = np.load('/media/pigz2538/e30413cf-5429-46b3-8fe9-ea458052c437/文档/tbmodel/batch4out.npy')\n",
    "\n",
    "batch4in.shape\n",
    "# batch1out.shape\n",
    "# batch4out[2940:]-batch1out\n",
    "# orb2_indexs[0][1]\n",
    "# orb2_indexk[0][1]\n",
    "s = batch4in[2940:]-batch1in\n",
    "s\n",
    "# d4[0][:] - d1[3]\n",
    "# batch1out[:,0]\n",
    "# batch4out[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.1024],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.3084],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.5068],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0163],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.1542],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.1254],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.4195],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [-0.0094],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.2534],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [-0.7070],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[1] - d4[0][980:1960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ -7.4597,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -5.3149,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -8.7348,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         -13.0381,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         -12.7598,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -3.1231,  -8.7043,   0.0000,   0.0000,   0.0000,   0.0000,  -4.6886,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  20.3012,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000, -11.8034,  -8.6950,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  -4.8134,   0.0000,   0.0000,   0.0000,   0.0000,  18.0668,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000, -12.0952], device='cuda:0',\n",
       "        grad_fn=<CopySlices>),\n",
       " tensor([ -2.4078,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -1.4331,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -7.2243,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -9.3098,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -6.2938,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -0.5179,  -8.4542,   0.0000,   0.0000,   0.0000,   0.0000,  -4.7906,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  21.8067,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000, -10.7924,  -5.0305,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  -2.4347,   0.0000,   0.0000,   0.0000,   0.0000,  11.0438,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  -4.8351], device='cuda:0',\n",
       "        grad_fn=<CopySlices>),\n",
       " tensor([ -8.9278,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -3.0240,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -8.2008,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         -11.8882,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         -13.0035,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -0.7324,  -8.4446,   0.0000,   0.0000,   0.0000,   0.0000,  -2.9670,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  13.7208,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,  -6.6708,  -6.6613,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  -3.2063,   0.0000,   0.0000,   0.0000,   0.0000,  13.3196,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  -7.8280], device='cuda:0',\n",
       "        grad_fn=<CopySlices>),\n",
       " tensor([ -7.3241,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.6257,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -6.9064,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         -16.3088,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         -20.3126,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          -1.6785,  -4.8871,   0.0000,   0.0000,   0.0000,   0.0000,  -4.1025,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  22.1702,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,  -6.6195,  -4.6964,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  -3.2592,   0.0000,   0.0000,   0.0000,   0.0000,  15.5721,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  -4.8231], device='cuda:0',\n",
       "        grad_fn=<CopySlices>)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envelope(nn.Module):  # bessel扩展计算式子\n",
    "\n",
    "    def __init__(self, exponent):\n",
    "        super(Envelope, self).__init__()\n",
    "\n",
    "        self.p = exponent + 1\n",
    "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
    "        self.b = self.p * (self.p + 2)\n",
    "        self.c = -self.p * (self.p + 1) / 2\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_p_0 = x.pow(self.p)\n",
    "        x_p_1 = x_p_0 * x\n",
    "        x_p_2 = x_p_1 * x\n",
    "        env_val = x + self.a * x_p_0 + self.b * x_p_1 + self.c * x_p_2\n",
    "\n",
    "        return env_val\n",
    "    \n",
    "class BesselBasisLayer(nn.Module): # bessel扩展\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_radial,\n",
    "                 cutoff = 20.,\n",
    "                 envelope_exponent = 6,):\n",
    "        super(BesselBasisLayer, self).__init__()\n",
    "\n",
    "        self.cutoff = cutoff\n",
    "        self.envelope = Envelope(envelope_exponent)\n",
    "        self.frequencies = nn.Parameter(torch.Tensor(num_radial))\n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "         \n",
    "         self.frequencies.data = torch.arange(1., self.frequencies.numel() + 1.\n",
    "                     ).mul_(np.pi)\n",
    "\n",
    "    def forward(self, d):\n",
    "\n",
    "        d_scaled = d / self.cutoff\n",
    "        d_cutoff = self.envelope(d_scaled)\n",
    "        d_sin = d_cutoff * torch.sin(self.frequencies * d_scaled)\n",
    "        # print(self.frequencies, d_cutoff.shape, d_scaled.shape, d_sin.shape)\n",
    "        return d_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expander = BesselBasisLayer(4, 20, 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = torch.tensor([1.1,2.2,3.3,4.4]).to(device)\n",
    "d1 = torch.tensor([1.55,5.1446,6.212,9.6868]).to(device)\n",
    "d2 = torch.tensor([8.8668,2.13,3.445,1.78]).to(device)\n",
    "d3 = torch.tensor([7.82,8.123,2.45785,4.4545]).to(device)\n",
    "d4 = torch.tensor([6.12,5.5457,7.785,9.545]).to(device)\n",
    "\n",
    "dfull = torch.cat([d1,d2,d3,d4]).to(device)\n",
    "a = expander(dfull.reshape(-1,1)).unsqueeze(-1)\n",
    "b = expander(d4.reshape(-1,1)).unsqueeze(-1)\n",
    "a[12:] - b\n",
    "# c = ex * d\n",
    "# c\n",
    "# c.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3,4],[4,3,2,1]])\n",
    "b = torch.tensor([[6,7,8,9],[8,9,7,6],[1,2,3,4],[4,3,2,1]])\n",
    "\n",
    "temp_feat = []\n",
    "for i in range(10):\n",
    "    temp_feat.append(a)\n",
    "\n",
    "d = torch.cat(temp_feat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m             rvectors[:, index[i]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(pk, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (dim_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rvectors\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrvector_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mrvector_init\u001b[0;34m(dim)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrvector_init\u001b[39m(dim):\n\u001b[0;32m----> 2\u001b[0m     dim_num \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39msum(dim)\n\u001b[1;32m      3\u001b[0m     rvectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m dim_num, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      4\u001b[0m     pk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def rvector_init(dim):\n",
    "    dim_num = np.sum(dim)\n",
    "    rvectors = np.zeros((3 ** dim_num, 3), dtype=int)\n",
    "    pk = np.arange(-1, 2, dtype=int)\n",
    "    index = np.where(dim==1)[0]\n",
    "    indexnum = np.arange(0, index.size)\n",
    "\n",
    "    for i in indexnum:\n",
    "        if i == 0:\n",
    "            rvectors[:, index[i]]= np.repeat(pk, 3 ** (dim_num - 1))\n",
    "        elif i == 1:\n",
    "            rvectors[:, index[i]] = np.repeat(np.tile(pk, 3), 3 ** (dim_num - 2))\n",
    "        elif i == 2:\n",
    "            rvectors[:, index[i]] = np.tile(pk, 3 ** (dim_num - 1))\n",
    "    return rvectors\n",
    "\n",
    "print(rvector_init(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['G', 'M', 'K', 'G']\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route = \"GMKG\"\n",
    "\n",
    "str(list(route))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
